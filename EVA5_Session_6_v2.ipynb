{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA5 - Session 6_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thamizhannal/eva5/blob/master/EVA5_Session_6_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7OU46ANBvfB",
        "colab_type": "text"
      },
      "source": [
        "###Target:\n",
        "Reach 99.4% accuracy consistently in last 3-4 epochs by a applying more regularization in the form of dropout and increase LR rate to understand if convergence occurs faster and use LR schedular to change LR wherever Loss gets to plateau\n",
        "\n",
        "###Results:\n",
        "1. Parameters: 7,917\n",
        "2. Best Train Accuracy: 99.08%\n",
        "3. Best Test Accuracy: 99.43% \n",
        "4. greater than 99.4% in last 4 epochs(12-15)\n",
        "\n",
        "###Analysis:\n",
        "1. Dropout added the necessary regularization required to make model robust\n",
        "2. Though increased LR was helping in fast convergence but after 5-6 epochs it would tapper off implying a need for smaller LR so used a step LR schedule at after 4 epochs and decreased it to 0.8 times the previous LR\n",
        "\n",
        "###All the changes added to the base CNN architechture to acheive the target accuracy \n",
        "1. Input image normalization\n",
        "2. Image augmentation - rotation for Train data \n",
        "3. Batch Normalization\n",
        "4. Dropouts \n",
        "5. LR scheduler \n",
        "6. GAP\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzD_rXJQYFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reused below code from https://github.com/apple/ml-cifar-10-faster/blob/master/utils.py\n",
        "# GhostBatchNormalization\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        self.weight.data.fill_(1.0)\n",
        "        self.bias.data.fill_(0.0)\n",
        "        self.weight.requires_grad = weight\n",
        "        self.bias.requires_grad = bias\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False):  # lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "        return super().train(mode)\n",
        "\n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return F.batch_norm(\n",
        "                input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var,\n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W)\n",
        "        else:\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],\n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "            "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_splits = 2\n",
        "#Receptive field is calcualted based on actual formula\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, enable_bn=True ):\n",
        "        super(Net, self).__init__()\n",
        "        self.enable_bn = enable_bn\n",
        "\n",
        "        #block1\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3,padding = True) #o/p size:28; rf: 3\n",
        "        self.Batchnorm1 = nn.BatchNorm2d(8)\n",
        "        if not enable_bn:\n",
        "            self.Batchnorm1 = GhostBatchNorm(num_features=8, num_splits=num_splits)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(8, 16, 3) #o/p size: 26; rf: 5\n",
        "        self.Batchnorm2 = nn.BatchNorm2d(16)\n",
        "        if not enable_bn:\n",
        "            self.Batchnorm2 = GhostBatchNorm(num_features=16, num_splits=num_splits)\n",
        "\n",
        "\n",
        "        #transition block\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #o/p size: 13; rf: 6\n",
        "        self.pool1trns = nn.Conv2d(16, 10, 1)#o/p size: 13; rf: 6 \n",
        "        self.Batchnormtrns1 = nn.BatchNorm2d(10)\n",
        "        if not enable_bn:\n",
        "            self.Batchnormtrns1 = GhostBatchNorm(num_features=10, num_splits=num_splits)\n",
        "        \n",
        "        #block2\n",
        "        self.conv3 = nn.Conv2d(10,14,3) #o/p size: 11; rf: 10 # 14\n",
        "        self.Batchnorm3 = nn.BatchNorm2d(14)\n",
        "        if not enable_bn:\n",
        "            self.Batchnorm3 = GhostBatchNorm(num_features=14, num_splits=num_splits)\n",
        "\n",
        "        self.dp3 = nn.Dropout(p = 0.10) \n",
        "        self.conv4 = nn.Conv2d(14, 16, 3) #o/p size: 9; rf: 14\n",
        "        self.Batchnorm4 = nn.BatchNorm2d(16)\n",
        "        if not enable_bn:\n",
        "            self.Batchnorm4 = GhostBatchNorm(num_features=16, num_splits=num_splits)\n",
        "\n",
        "        self.dp4 = nn.Dropout(p = 0.10)\n",
        "        self.conv5 = nn.Conv2d(16, 20, 3) #o/p size: 7; rf: 18\n",
        "        self.Batchnorm5 = nn.BatchNorm2d(20)\n",
        "        if not enable_bn:\n",
        "            self.Batchnorm5 = GhostBatchNorm(num_features=20, num_splits=num_splits)\n",
        "\n",
        "        self.dp5 = nn.Dropout(p = 0.10)\n",
        "\n",
        "        #gap and 1X1 \n",
        "        self.conv6_avgp = nn.AvgPool2d(kernel_size=7) #o/p size: 1; rf: 30\n",
        "        self.pool2trns = nn.Conv2d(20,10,1) #6\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnorm1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnorm2(x)\n",
        "\n",
        "\n",
        "        x = self.pool1(x)\n",
        "        x = self.pool1trns(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnormtrns1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnorm3(x)\n",
        "        x = self.dp3(x)\n",
        " \n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnorm4(x)\n",
        "        x = self.dp4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Batchnorm5(x)\n",
        "        x = self.dp5(x)\n",
        "\n",
        "        x = self.conv6_avgp(x)\n",
        "\n",
        "        x = self.pool2trns(x)\n",
        "\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x,dim=-1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "43da5f64-2519-4d31-a90a-2666d7824c74"
      },
      "source": [
        "#!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net(True).to(device)\n",
        "\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "            Conv2d-3           [-1, 16, 26, 26]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 26, 26]              32\n",
            "         MaxPool2d-5           [-1, 16, 13, 13]               0\n",
            "            Conv2d-6           [-1, 10, 13, 13]             170\n",
            "       BatchNorm2d-7           [-1, 10, 13, 13]              20\n",
            "            Conv2d-8           [-1, 14, 11, 11]           1,274\n",
            "       BatchNorm2d-9           [-1, 14, 11, 11]              28\n",
            "          Dropout-10           [-1, 14, 11, 11]               0\n",
            "           Conv2d-11             [-1, 16, 9, 9]           2,032\n",
            "      BatchNorm2d-12             [-1, 16, 9, 9]              32\n",
            "          Dropout-13             [-1, 16, 9, 9]               0\n",
            "           Conv2d-14             [-1, 20, 7, 7]           2,900\n",
            "      BatchNorm2d-15             [-1, 20, 7, 7]              40\n",
            "          Dropout-16             [-1, 20, 7, 7]               0\n",
            "        AvgPool2d-17             [-1, 20, 1, 1]               0\n",
            "           Conv2d-18             [-1, 10, 1, 1]             210\n",
            "================================================================\n",
            "Total params: 8,002\n",
            "Trainable params: 8,002\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.40\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.43\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 64\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.RandomRotation((-8.0, 8.0), fill=(1,)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch, \n",
        "          enable_l1=False, enable_l2=False, enable_gbn=False):\n",
        "    model.train()\n",
        "    #pbar = tqdm(train_loader)\n",
        "    train_loss = 0 \n",
        "    train_correct = 0\n",
        "    # For best lambda value, apply grid search\n",
        "    lambda_l1 = 0.001\n",
        "    #scheduler.step()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # loss function\n",
        "        # L1 loss\n",
        "        if (enable_l1 and not (enable_l2 and enable_gbn)):\n",
        "            l1_loss = 0\n",
        "            for param in model.parameters():\n",
        "                l1_loss = l1_loss + param.abs().sum()\n",
        "            loss = loss + lambda_l1 * l1_loss\n",
        "        # L2 loss    \n",
        "        #elif enable_l2 and not (enable_l1 and enable_gbn):\n",
        "            #print (\"LOSS:l2\")\n",
        "        # L1L2 loss\n",
        "        #elif enable_l1 and enable_l2 and not enable_gbn:\n",
        "            #print(\"l1l2\")\n",
        "        # GBN only\n",
        "        #elif enable_gbn and not (enable_l1 and enable_l2) :  \n",
        "            #print(\"gbn only\")\n",
        "        # L1L2&GBN loss\n",
        "        #elif enable_gbn and enable_l1 and enable_l2 :  \n",
        "            #print(\"l1l2gbn\")\n",
        "        #\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "        train_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    train_loss /= len(test_loader.dataset)\n",
        "    print('Epoch: {:.0f},LR: {}.\\nTrain set: train Average loss: {:.4f}, train_Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        epoch,optimizer.param_groups[0]['lr'],train_loss, train_correct, len(train_loader.dataset),\n",
        "        100. * train_correct / len(train_loader.dataset)))\n",
        "        \n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "            pred_test = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred_test.eq(target.view_as(pred_test)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    \n",
        "    print('Test set: test Average loss: {:.4f}, test Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2ca520c-98d4-4dfc-96b3-24669b9f1e88"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Loop To Run Network \n",
        "# 1. with L1 + BN\n",
        "# 2. with L2 + BN\n",
        "# 3. with L1 and L2 with BN\n",
        "# 4. with GBN\n",
        "# 5. with L1 and L2 with GBN\n",
        "\n",
        "losses = ['l1', 'l2', 'l1&l2', 'gbn', 'l1l2gbn']\n",
        "\n",
        "for idx in range(len(losses))\n",
        "    i = idx+1\n",
        "    enable_l1 = False\n",
        "    enable_l2 = False\n",
        "    enable_gbn = False\n",
        "\n",
        "    print(\"{} {} loss\".format((i),losses[idx]))\n",
        "    ######### Model Initialization #############################################\n",
        "    # Enable BN for option 1, 2 & 3\n",
        "    if i == 1 or i==2 or i==3:\n",
        "        model = Net(enable_bn=True).to(device)\n",
        "        print(\"BN enabled for option: {}\".format(i))\n",
        "    # Enable GBN for option 4 & 5( disable BN)\n",
        "    elif i==4 or i==5:\n",
        "        model = Net(enable_bn=False).to(device)\n",
        "        print(\"GBN Enabled & BN disabled for option: {}\".format(i))\n",
        "    print (\"Model Summary:\")\n",
        "    summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "    ########## Optimizer Initialization #######################################\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
        "    if i==2 or i==3 or i==5:\n",
        "        # decay of 0.0001 gave best val accuracy 99.38\n",
        "        decay = 0.0001\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=decay)  \n",
        "        print(\"Weight decay {} set for option: {}\".format(decay, i))\n",
        "    \n",
        "    scheduler = StepLR(optimizer, step_size=4, gamma=0.8)\n",
        "    ######### Loss Function Selection #########################################\n",
        "    if i==1:\n",
        "         enable_l1 = True\n",
        "         print(\"LOSS: l1 loss enabled\")\n",
        "    elif i==2:\n",
        "         enable_l2 = True\n",
        "         print(\"LOSS: l2 loss enabled\")\n",
        "    elif i==3:\n",
        "         enable_l1 = True\n",
        "         enable_l2=True\n",
        "         print(\"LOSS: l1l2 loss enabled\")\n",
        "    elif i==4:         \n",
        "         enable_gbn = True\n",
        "         print(\"LOSS: gbn loss enabled\")\n",
        "    elif i==5:\n",
        "         enable_l1 = True\n",
        "         enable_l2=True\n",
        "         enable_gbn = True\n",
        "         print(\"LOSS: l1l2gbn loss enabled\")\n",
        "\n",
        "    for epoch in range(1, 25):\n",
        "        train(model, device, train_loader, optimizer, epoch, \n",
        "              enable_l1=enable_l1, enable_l2=enable_l2, enable_gbn=enable_gbn)\n",
        "        scheduler.step()\n",
        "        test(model, device, test_loader)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 l1 loss\n",
            "BN enabled for option: 1\n",
            "Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "            Conv2d-3           [-1, 16, 26, 26]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 26, 26]              32\n",
            "         MaxPool2d-5           [-1, 16, 13, 13]               0\n",
            "            Conv2d-6           [-1, 10, 13, 13]             170\n",
            "       BatchNorm2d-7           [-1, 10, 13, 13]              20\n",
            "            Conv2d-8           [-1, 14, 11, 11]           1,274\n",
            "       BatchNorm2d-9           [-1, 14, 11, 11]              28\n",
            "          Dropout-10           [-1, 14, 11, 11]               0\n",
            "           Conv2d-11             [-1, 16, 9, 9]           2,032\n",
            "      BatchNorm2d-12             [-1, 16, 9, 9]              32\n",
            "          Dropout-13             [-1, 16, 9, 9]               0\n",
            "           Conv2d-14             [-1, 20, 7, 7]           2,900\n",
            "      BatchNorm2d-15             [-1, 20, 7, 7]              40\n",
            "          Dropout-16             [-1, 20, 7, 7]               0\n",
            "        AvgPool2d-17             [-1, 20, 1, 1]               0\n",
            "           Conv2d-18             [-1, 10, 1, 1]             210\n",
            "================================================================\n",
            "Total params: 8,002\n",
            "Trainable params: 8,002\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.40\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.43\n",
            "----------------------------------------------------------------\n",
            "LOSS: l1 loss enabled\n",
            "Epoch: 1,LR: 0.03.\n",
            "Train set: train Average loss: 1.7045, train_Accuracy: 54994/60000 (91.6567%)\n",
            "\n",
            "Test set: test Average loss: 0.1028, test Accuracy: 9704/10000 (97.0400%)\n",
            "\n",
            "Epoch: 2,LR: 0.03.\n",
            "Train set: train Average loss: 0.8036, train_Accuracy: 57597/60000 (95.9950%)\n",
            "\n",
            "Test set: test Average loss: 0.0995, test Accuracy: 9706/10000 (97.0600%)\n",
            "\n",
            "Epoch: 3,LR: 0.03.\n",
            "Train set: train Average loss: 0.8005, train_Accuracy: 57561/60000 (95.9350%)\n",
            "\n",
            "Test set: test Average loss: 0.3298, test Accuracy: 8909/10000 (89.0900%)\n",
            "\n",
            "Epoch: 4,LR: 0.03.\n",
            "Train set: train Average loss: 0.7731, train_Accuracy: 57625/60000 (96.0417%)\n",
            "\n",
            "Test set: test Average loss: 0.1736, test Accuracy: 9424/10000 (94.2400%)\n",
            "\n",
            "Epoch: 5,LR: 0.024.\n",
            "Train set: train Average loss: 0.7225, train_Accuracy: 57843/60000 (96.4050%)\n",
            "\n",
            "Test set: test Average loss: 0.2565, test Accuracy: 9245/10000 (92.4500%)\n",
            "\n",
            "Epoch: 6,LR: 0.024.\n",
            "Train set: train Average loss: 0.7334, train_Accuracy: 57784/60000 (96.3067%)\n",
            "\n",
            "Test set: test Average loss: 0.2615, test Accuracy: 9129/10000 (91.2900%)\n",
            "\n",
            "Epoch: 7,LR: 0.024.\n",
            "Train set: train Average loss: 0.6977, train_Accuracy: 57952/60000 (96.5867%)\n",
            "\n",
            "Test set: test Average loss: 0.1913, test Accuracy: 9390/10000 (93.9000%)\n",
            "\n",
            "Epoch: 8,LR: 0.024.\n",
            "Train set: train Average loss: 0.7094, train_Accuracy: 57842/60000 (96.4033%)\n",
            "\n",
            "Test set: test Average loss: 0.2828, test Accuracy: 9100/10000 (91.0000%)\n",
            "\n",
            "Epoch: 9,LR: 0.019200000000000002.\n",
            "Train set: train Average loss: 0.6602, train_Accuracy: 58020/60000 (96.7000%)\n",
            "\n",
            "Test set: test Average loss: 0.1637, test Accuracy: 9489/10000 (94.8900%)\n",
            "\n",
            "Epoch: 10,LR: 0.019200000000000002.\n",
            "Train set: train Average loss: 0.6637, train_Accuracy: 57986/60000 (96.6433%)\n",
            "\n",
            "Test set: test Average loss: 0.0916, test Accuracy: 9700/10000 (97.0000%)\n",
            "\n",
            "Epoch: 11,LR: 0.019200000000000002.\n",
            "Train set: train Average loss: 0.6598, train_Accuracy: 58021/60000 (96.7017%)\n",
            "\n",
            "Test set: test Average loss: 0.1008, test Accuracy: 9673/10000 (96.7300%)\n",
            "\n",
            "Epoch: 12,LR: 0.019200000000000002.\n",
            "Train set: train Average loss: 0.6775, train_Accuracy: 57943/60000 (96.5717%)\n",
            "\n",
            "Test set: test Average loss: 0.2528, test Accuracy: 9171/10000 (91.7100%)\n",
            "\n",
            "Epoch: 13,LR: 0.015360000000000002.\n",
            "Train set: train Average loss: 0.6269, train_Accuracy: 58159/60000 (96.9317%)\n",
            "\n",
            "Test set: test Average loss: 0.1029, test Accuracy: 9654/10000 (96.5400%)\n",
            "\n",
            "Epoch: 14,LR: 0.015360000000000002.\n",
            "Train set: train Average loss: 0.6408, train_Accuracy: 58032/60000 (96.7200%)\n",
            "\n",
            "Test set: test Average loss: 0.0750, test Accuracy: 9779/10000 (97.7900%)\n",
            "\n",
            "Epoch: 15,LR: 0.015360000000000002.\n",
            "Train set: train Average loss: 0.6489, train_Accuracy: 58037/60000 (96.7283%)\n",
            "\n",
            "Test set: test Average loss: 0.0887, test Accuracy: 9738/10000 (97.3800%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvGPL-vSbvZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}